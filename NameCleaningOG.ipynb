{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cleaning the Names of the Data</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the library to read the excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the excel into the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Event</th>\n",
       "      <th>NAME</th>\n",
       "      <th>Age</th>\n",
       "      <th>TEAM</th>\n",
       "      <th>Result</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>m/s</th>\n",
       "      <th>Competition</th>\n",
       "      <th>Year D.O.B.</th>\n",
       "      <th>Info, if any</th>\n",
       "      <th>Date of Birth (PDPA)</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-14 00:00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Men High Jump</td>\n",
       "      <td>KAM, KAMPTON</td>\n",
       "      <td>22</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>2.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>International - Penn 10-Team Select</td>\n",
       "      <td>2000</td>\n",
       "      <td>Sanctioned,1pos</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-04 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Men Long Jump</td>\n",
       "      <td>TOH, WEI YU</td>\n",
       "      <td>-</td>\n",
       "      <td>NANYANG TECHNOLOGICAL UNI</td>\n",
       "      <td>6.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Local - IVP</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-04 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Men Long Jump</td>\n",
       "      <td>MUHAMAD NURSHAHILMI, BIN NORADEE</td>\n",
       "      <td>-</td>\n",
       "      <td>TEMASEK POLYTECHNIC</td>\n",
       "      <td>6.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Local - IVP</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-04 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Men Long Jump</td>\n",
       "      <td>SEBASTIAN, KIRAN</td>\n",
       "      <td>-</td>\n",
       "      <td>REPUBLIC POLYTECHNIC</td>\n",
       "      <td>6.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>Local - IVP</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-04 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Men Long Jump</td>\n",
       "      <td>NAING, KYAW LINN</td>\n",
       "      <td>-</td>\n",
       "      <td>SINGAPORE MANAGEMENT UNI</td>\n",
       "      <td>6.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Local - IVP</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122793</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-07-17 00:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Men 100 Meter Dash</td>\n",
       "      <td>Azhari Muhd Yusof</td>\n",
       "      <td>17</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>11.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>International - Asian Junior</td>\n",
       "      <td>1989</td>\n",
       "      <td>Semis, 7/8pos</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122794</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-07-17 00:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Women Discus Throw (1kg)</td>\n",
       "      <td>Wan, Lay Chi</td>\n",
       "      <td>18</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>44.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>International - Asian Junior</td>\n",
       "      <td>1988</td>\n",
       "      <td>Final, 6/14pos</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122795</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-07-17 00:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Women 100 Meter Dash</td>\n",
       "      <td>Ann Siao Mei</td>\n",
       "      <td>19</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>12.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3</td>\n",
       "      <td>International - Asian Junior</td>\n",
       "      <td>1987</td>\n",
       "      <td>R1, 5/7pos</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122796</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-07-17 00:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Women 100 Meter Dash</td>\n",
       "      <td>Choo Sze-Min, Amanda</td>\n",
       "      <td>19</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>12.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>International - Asian Junior</td>\n",
       "      <td>1987</td>\n",
       "      <td>R1, 4/6pos q</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122797</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-07-17 00:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Women 100 Meter Dash</td>\n",
       "      <td>Choo Sze-Min, Amanda</td>\n",
       "      <td>19</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>12.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>International - Asian Junior</td>\n",
       "      <td>1987</td>\n",
       "      <td>Final, 8/8pos</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122796 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                 Date  Day  Month    Year  \\\n",
       "0              NaN  2023-01-14 00:00:00   14    1.0  2023.0   \n",
       "1              NaN  2023-01-04 00:00:00    4    1.0  2023.0   \n",
       "2              NaN  2023-01-04 00:00:00    4    1.0  2023.0   \n",
       "3              NaN  2023-01-04 00:00:00    4    1.0  2023.0   \n",
       "4              NaN  2023-01-04 00:00:00    4    1.0  2023.0   \n",
       "...            ...                  ...  ...    ...     ...   \n",
       "122793         NaN  2006-07-17 00:00:00   17    7.0  2006.0   \n",
       "122794         NaN  2006-07-17 00:00:00   17    7.0  2006.0   \n",
       "122795         NaN  2006-07-17 00:00:00   17    7.0  2006.0   \n",
       "122796         NaN  2006-07-17 00:00:00   17    7.0  2006.0   \n",
       "122797         NaN  2006-07-17 00:00:00   17    7.0  2006.0   \n",
       "\n",
       "                           Event                              NAME Age  \\\n",
       "0                  Men High Jump                      KAM, KAMPTON  22   \n",
       "1                  Men Long Jump                       TOH, WEI YU   -   \n",
       "2                  Men Long Jump  MUHAMAD NURSHAHILMI, BIN NORADEE   -   \n",
       "3                  Men Long Jump                  SEBASTIAN, KIRAN   -   \n",
       "4                  Men Long Jump                  NAING, KYAW LINN   -   \n",
       "...                          ...                               ...  ..   \n",
       "122793        Men 100 Meter Dash                 Azhari Muhd Yusof  17   \n",
       "122794  Women Discus Throw (1kg)                      Wan, Lay Chi  18   \n",
       "122795      Women 100 Meter Dash                      Ann Siao Mei  19   \n",
       "122796      Women 100 Meter Dash              Choo Sze-Min, Amanda  19   \n",
       "122797      Women 100 Meter Dash              Choo Sze-Min, Amanda  19   \n",
       "\n",
       "                             TEAM Result  Unnamed: 10  m/s  \\\n",
       "0                      INDIVIDUAL   2.06          NaN    -   \n",
       "1       NANYANG TECHNOLOGICAL UNI   6.64          NaN  0.2   \n",
       "2             TEMASEK POLYTECHNIC   6.28          NaN  0.1   \n",
       "3            REPUBLIC POLYTECHNIC   6.27          NaN  1.7   \n",
       "4        SINGAPORE MANAGEMENT UNI   6.26          NaN  1.2   \n",
       "...                           ...    ...          ...  ...   \n",
       "122793                  Singapore  11.12          NaN -0.4   \n",
       "122794                  Singapore  44.85          NaN    -   \n",
       "122795                  Singapore  12.93          NaN   -3   \n",
       "122796                  Singapore  12.39          NaN  0.2   \n",
       "122797                  Singapore  12.39          NaN -0.8   \n",
       "\n",
       "                                Competition Year D.O.B.     Info, if any  \\\n",
       "0       International - Penn 10-Team Select        2000  Sanctioned,1pos   \n",
       "1                               Local - IVP           -                -   \n",
       "2                               Local - IVP           -                -   \n",
       "3                               Local - IVP           -                -   \n",
       "4                               Local - IVP           -                -   \n",
       "...                                     ...         ...              ...   \n",
       "122793         International - Asian Junior        1989    Semis, 7/8pos   \n",
       "122794         International - Asian Junior        1988   Final, 6/14pos   \n",
       "122795         International - Asian Junior        1987       R1, 5/7pos   \n",
       "122796         International - Asian Junior        1987     R1, 4/6pos q   \n",
       "122797         International - Asian Junior        1987    Final, 8/8pos   \n",
       "\n",
       "       Date of Birth (PDPA) Unnamed: 16  \n",
       "0                         -         NaN  \n",
       "1                         -         NaN  \n",
       "2                         -         NaN  \n",
       "3                         -         NaN  \n",
       "4                         -         NaN  \n",
       "...                     ...         ...  \n",
       "122793                    -         NaN  \n",
       "122794                    -         NaN  \n",
       "122795                    -         NaN  \n",
       "122796                    -         NaN  \n",
       "122797                    -         NaN  \n",
       "\n",
       "[122796 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.read_excel('RawData/Ranking.xlsx', sheet_name='Consolidated2')\n",
    "df = df[df['NAME'].notna()]\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split all names into first and last name, remove the relays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffnln=df.copy()\n",
    "dffnln=dffnln.loc[~(dffnln['Event'].isin(['Women 4x400 Meter Relay', ' Men 4x400 Meter Relay', 'Men 4x400 Meter Relay', 'Mixed 4x400 Meter Relay', 'Women 4x100 Meter Relay', 'Men 4x100 Meter Relay', 'Mixed 4x100 Meter Relay']))]\n",
    "dffnln.Event.unique()\n",
    "\n",
    "dffnln['FirstName'] = dffnln['NAME'].str.split(',').str[0]\n",
    "dffnln['LastName'] = dffnln['NAME'].str.split(',').str[-1]\n",
    "\n",
    "\n",
    "\n",
    "# writer = pd.ExcelWriter('Checkpoints.xlsx', engine = 'openpyxl')\n",
    "# dffnln.to_excel(writer, sheet_name = 'FirstNameLastName')\n",
    "# writer.close()\n",
    "\n",
    "# dffnln.to_csv('Checkpoint1.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning to check the similarities between names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5163977794943222"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    # print vec1, vec2\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    return Counter(WORD.findall(text))\n",
    "\n",
    "def get_similarity(a, b):\n",
    "    a = text_to_vector(a.strip().lower())\n",
    "    b = text_to_vector(b.strip().lower())\n",
    "\n",
    "    return get_cosine(a, b)\n",
    "\n",
    "get_similarity('TAN ELIZABETH-A', 'TAN ELIZABETH ANN SHEE RU') # returns 0.9258200997725514"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See all similar names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(map(lambda v: v in ('BIN ZAINAL MUHAMMAD FAZREY'.lower()).split(' '), ['mohamad', 'muhammad', 'mohammad', 'muhammad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToCheck={}\n",
    "# dfname=dffnln.copy()\n",
    "# # dfname=dfname.drop_duplicates(subset=['NAME'])\n",
    "# # dfname.to_csv('dfname.csv')\n",
    "\n",
    "# counter=0\n",
    "\n",
    "# for firstname, lastname in zip(dfname['FirstName'], dfname['LastName']):\n",
    "#     counter+=1\n",
    "#     for keys in ToCheck.keys():\n",
    "#         i, j=keys.split('/')[0], keys.split('/')[-1]\n",
    "#         score1=get_similarity(firstname,i)\n",
    "#         score2=get_similarity(lastname, j)\n",
    "#         if any(map(lambda v: v in (firstname.lower()+lastname.lower()).split(' '), ['mohamad', 'muhammad', 'mohammad', 'muhammad'])): \n",
    "#             # print(firstname, lastname)\n",
    "#             if score1>0.7 and score2>0.7:\n",
    "#                 ToCheck[keys]+=[f'{firstname}/{lastname} ({round(score1, 2), round(score2, 2)})']\n",
    "#                 # print(f'{firstname} // {i}')\n",
    "#                 break\n",
    "#             else:\n",
    "#                 scoreboth=get_similarity(firstname+lastname,keys)\n",
    "#                 if scoreboth>0.7:\n",
    "#                     ToCheck[keys]+=[f'{firstname}/{lastname} ({round(scoreboth, 2)})']\n",
    "#                     break\n",
    "#         else:\n",
    "#             if score1>0.5 and score2>0.5:\n",
    "#                 ToCheck[keys]+=[f'{firstname}/{lastname} ({round(score1, 2), round(score2, 2)})']\n",
    "#                 # print(f'{firstname} // {i}')\n",
    "#                 break\n",
    "#             else:\n",
    "#                 scoreboth=get_similarity(firstname+lastname,keys)\n",
    "#                 if scoreboth>0.5:\n",
    "#                     ToCheck[keys]+=[f'{firstname}/{lastname} ({round(scoreboth, 2)})']\n",
    "#                     break\n",
    "#     else:\n",
    "#         ToCheck[(firstname+'/'+lastname).upper()]=[]\n",
    "\n",
    "#     if counter%500==0:\n",
    "#         print(f'{counter} out of {len(dfname)} completed.')\n",
    "\n",
    "# pd.DataFrame(pd.Series(ToCheck)).to_csv('Checkpoint6.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UniqueNames=[''] ##Can consider dropping the mohammad when comparing or dropping punctuation\n",
    "# counter=0\n",
    "# checkdf=dffnln.copy()\n",
    "# checkdf=checkdf.drop_duplicates(subset=['NAME'])\n",
    "# checkdf.to_csv('checkpoint3.csv')\n",
    "\n",
    "\n",
    "# for name, team, age in zip(checkdf['NAME'], checkdf['TEAM'], checkdf['Age']):\n",
    "#     # print(i)\n",
    "#     counter+=1\n",
    "#     for j in UniqueNames:\n",
    "#         score=get_similarity(name,j)\n",
    "#         if 'MOHAMMAD' in name:\n",
    "#             if score > 0.65: #Check if the names is already in the UniqueNames, if similarity score >60% means same name\n",
    "#                 if len(name)>=len(j): #Take the longer name for the Unique Names\n",
    "#                     # with open(\"output.txt\", \"a\") as f:\n",
    "#                     #     print(f\"Replacing {j} ({len(j)}) with {i} ({len(i)}). {counter}\", file=f)\n",
    "#                     # UniqueNameReplace=bool(input(f\"Replace {name.upper()} with {j.upper()}.?\"))\n",
    "#                     # if UniqueNameReplace:\n",
    "#                     UniqueNames[UniqueNames.index(j)]=name\n",
    "#                 break\n",
    "#         else:\n",
    "#             if score > 0.70: #Check if the names is already in the UniqueNames, if similarity score >60% means same name\n",
    "#                 if len(name)>=len(j): #Take the longer name for the Unique Names\n",
    "#                     # with open(\"output.txt\", \"a\") as f:\n",
    "#                     #     print(f\"Replacing {j} ({len(j)}) with {i} ({len(i)}). {counter}\", file=f)\n",
    "#                     # UniqueNameReplace=bool(input(f\"Replace {name.upper()} with {j.upper()}.?\"))\n",
    "#                     # if UniqueNameReplace:\n",
    "#                     UniqueNames[UniqueNames.index(j)]=name\n",
    "#                 break\n",
    "            \n",
    "#     else:\n",
    "#         # with open(\"output.txt\", \"a\") as f:\n",
    "#         #     print(f\"Adding {i} into Unique Names.\", file=f)\n",
    "\n",
    "#         UniqueNames.append(name)\n",
    "    \n",
    "#     if counter%500==0:\n",
    "#         print(f'{counter} out of {len(checkdf)} done.')\n",
    "\n",
    "# print(len(UniqueNames))\n",
    "# print(ToCheck)\n",
    "\n",
    "# Checking=pd.DataFrame(UniqueNames)\n",
    "# Checking.to_csv('Checkpoint5.csv')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UniqueNames={'Name': 'Team'}\n",
    "# counter=0\n",
    "# checkdf=dffnln.copy()\n",
    "# checkdf=checkdf.drop_duplicates(subset=['NAME'])\n",
    "# checkdf.to_csv('checkpoint3.csv')\n",
    "# for name, team, age in zip(checkdf['NAME'], checkdf['TEAM'], checkdf['Age']):\n",
    "#     # print(i)\n",
    "#     counter+=1\n",
    "#     for j in UniqueNames:\n",
    "#         score=get_similarity(name,j)\n",
    "#         if 'MOHAMMAD' in name:\n",
    "#             if score > 0.65 and team==UniqueNames[j]: #Check if the names is already in the UniqueNames, if similarity score >60% means same name\n",
    "#                 if len(name)>=len(j): #Take the longer name for the Unique Names\n",
    "#                     # with open(\"output.txt\", \"a\") as f:\n",
    "#                     #     print(f\"Replacing {j} ({len(j)}) with {i} ({len(i)}). {counter}\", file=f)\n",
    "#                     UniqueNameReplace=bool(input(f\"Replace {name.upper()} with {j.upper()}.?\"))\n",
    "#                     if UniqueNameReplace:\n",
    "#                         UniqueNames[j]=name.upper()\n",
    "\n",
    "#                 break\n",
    "#         else:\n",
    "#             if score > 0.70: #Check if the names is already in the UniqueNames, if similarity score >60% means same name\n",
    "#                 if len(name)>=len(j) and team==UniqueNames[j]: #Take the longer name for the Unique Names\n",
    "#                     # with open(\"output.txt\", \"a\") as f:\n",
    "#                     #     print(f\"Replacing {j} ({len(j)}) with {i} ({len(i)}). {counter}\", file=f)\n",
    "#                     UniqueNameReplace=bool(input(f\"Replace {name.upper()} with {j.upper()}.?\"))\n",
    "#                     if UniqueNameReplace:\n",
    "#                         UniqueNames[j]=name.upper()\n",
    "\n",
    "#                 break\n",
    "            \n",
    "#     else:\n",
    "#         # with open(\"output.txt\", \"a\") as f:\n",
    "#         #     print(f\"Adding {i} into Unique Names.\", file=f)\n",
    "\n",
    "#         UniqueNames[name]=team\n",
    "    \n",
    "#     if counter%500==0:\n",
    "#         print(f'{counter} out of {len(checkdf)} done.')\n",
    "\n",
    "# print(len(UniqueNames))\n",
    "\n",
    "# Checking=pd.DataFrame({'Name':[i for i in UniqueNames.keys()], 'Team': [j for j in UniqueNames.values()]})\n",
    "# Checking.to_csv('Checkpoint5.csv')\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfUniqueName=dffnln.copy()\n",
    "# EditedNames=[]\n",
    "# counter=0\n",
    "\n",
    "# for i in dfUniqueName['NAME']:\n",
    "#     counter+=1\n",
    "#     # print(i)\n",
    "#     for j in UniqueNames:\n",
    "#         score=get_similarity(i,j)\n",
    "#         if 'MOHAMMAD' in i:\n",
    "#             if score > 0.65: #Check if the names is already in the UniqueNames, if similarity score >60% means same name\n",
    "#                 EditedNames.append(j)\n",
    "#                 break\n",
    "#         else:\n",
    "#             if score > 0.55:\n",
    "#                 EditedNames.append(j)\n",
    "#                 break\n",
    "#     else:\n",
    "#         EditedNames.append('No Match')\n",
    "    \n",
    "#     # print(f'i is {i}, j is {j}')\n",
    "\n",
    "#     if counter%500==0:\n",
    "#         print(f'{counter} out of {len(dfUniqueName)} done.')\n",
    "\n",
    "# dfUniqueName['Edited Names']=EditedNames\n",
    "\n",
    "# dfUniqueName.to_csv('Checkpoint4.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign UniqueID to all names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UniqueNames={}\n",
    "# # counter=0\n",
    "# # for key,value in dffnln['NAME'].items():\n",
    "# #     if value not in UniqueNames:\n",
    "# #         UniqueNames[value]=counter\n",
    "# #         counter+=1\n",
    "\n",
    "# dffnln['UniqueID']=(dffnln['FirstName'].str[:2]+dffnln['LastName'].str[:2]+dffnln['Year D.O.B.'].astype(str)).str.replace(' ', '')    \n",
    "# # writer = pd.ExcelWriter('Checkpoints.xlsx', engine = 'openpyxl')\n",
    "# # dffnln.to_excel(writer, sheet_name = 'UniqueID')\n",
    "# # writer.close()\n",
    "\n",
    "# dffnln.to_csv('Checkpoint2.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UniqueNames=pd.read_csv('UniqueNames.csv')\n",
    "# Changer=[]\n",
    "\n",
    "# UniqueNames.columns=['Name', 'Edit']\n",
    "# # check=UniqueNames['Edit'].tolist()\n",
    "# # Changer=[]\n",
    "# for names in UniqueNames['Edit']:\n",
    "#     # print(names)\n",
    "#     newname=re.sub( \"\\s*\\(.*?\\)\\s*\",\" \",names)\n",
    "#     newname=newname.replace('[]','')\n",
    "#     newname=newname.replace('[','')\n",
    "#     newname=newname.replace(']', '')\n",
    "#     newname=newname.replace('(', '')\n",
    "#     newname=newname.replace(')', '')\n",
    "#     newname=newname.replace(\"'\", '')\n",
    "#     newname=newname.replace(\"'\", '')\n",
    "#     newname=newname.replace('/', '')\n",
    "#     Changer.append(newname)\n",
    "#     # print(newname)\n",
    "#     # Changer.append(re.sub( \"\\s*\\(.*?\\)\\s*\",\" \",names))\n",
    "#     # Changer.append(name.replace('[]', ''))\n",
    "\n",
    "# UniqueNames['NameFix']=UniqueNames['Name'].replace(['/', \"'\", '(', ')'],'')\n",
    "# UniqueNames['ChangetoThese']=Changer\n",
    "# UniqueNames.to_csv('UniqueNamesEdited.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namedb=pd.read_csv('UniqueNamesRev13.csv')\n",
    "# Namedb=Namedb.fillna('EMPTY')\n",
    "# # newdf=pd.read_csv('dfname.csv')\n",
    "# newdf=dffnln.copy()\n",
    "# toreplace=[]\n",
    "# formetocheck={}\n",
    "# counter=0\n",
    "\n",
    "# # for k in Namedb.keys():\n",
    "# #     print(Namedb[k])\n",
    "\n",
    "# # print(True in Namedb['TAN ELIZABETH ANN SHEE RU'].str.contains('TAN ELIZABETH ANN SHEE RU' ))\n",
    "# # for i in Namedb.keys():\n",
    "# #     print('KAM KAMPTON' == i)\n",
    "# # print('SO GOTHANDARAMAN BHUVANESH' in Namedb.items())\n",
    "\n",
    "# # for k, v in Namedb.items():\n",
    "# #     if 'SO GOTHANDARAMAN/ BHUVANESH' in v: \n",
    "# #         print('yes')\n",
    "# #         break\n",
    "\n",
    "# # print('KAM KAMPTON' in Namedb.keys())\n",
    "\n",
    "# newdf['CheckNames']=newdf['Name'].replace(',', '', regex=True)\n",
    "# newdf['CheckNames']=newdf['Name'].replace(\"'\", '', regex=True)\n",
    "# newdf['CheckNames']=newdf['CheckNames'].str.upper()\n",
    "# newdf['CheckNames']=newdf['CheckNames'].str.strip()\n",
    "# # print(newdf['CheckNames'])\n",
    "# for names in newdf['CheckNames']:\n",
    "#     names=names.replace(',', '')\n",
    "#     names=names.strip()\n",
    "#     for k in Namedb.keys():\n",
    "#         try:\n",
    "#             if names.upper()==k:\n",
    "#                 toreplace.append(k)\n",
    "#                 formetocheck[names]=k\n",
    "#                 break\n",
    "#             elif (Namedb[k].str.match(names.upper()).any()): #please remove all the unnamed rows if get the .str accessor error\n",
    "#                 toreplace.append(k)\n",
    "#                 formetocheck[names]=k\n",
    "#                 break\n",
    "#         except:\n",
    "#             toreplace.append('Error')\n",
    "#             formetocheck[names]='Error'\n",
    "#             break\n",
    "#     else:\n",
    "#         toreplace.append('No Match')\n",
    "#         formetocheck[names]='No Match'\n",
    "#         # print(names)\n",
    "\n",
    "#     counter+=1\n",
    "#     if counter%500==0:\n",
    "#         print(f'{counter} out of {len(newdf[\"Name\"])} done.')\n",
    "\n",
    "# newdf['NAME EDITED']=toreplace\n",
    "# newdf.to_csv('2014.csv')\n",
    " \n",
    "# #Just run this and wait 30 mins to see if it works\n",
    "\n",
    "# # Namedb.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(toreplace).to_csv('ToReplace.csv')\n",
    "# pd.DataFrame(formetocheck).to_csv('ForMeToCheck.csv')\n",
    "\n",
    "# newdf.to_csv('2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'UniqueNamesRev24.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m UniqueNames\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_excel(\u001b[39m'\u001b[39;49m\u001b[39mUniqueNamesRev24.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m UniqueNames\u001b[39m=\u001b[39mUniqueNames\u001b[39m.\u001b[39mfillna(\u001b[39m'\u001b[39m\u001b[39mNIL\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m UniqueNames\u001b[39m.\u001b[39mreset_index()\n",
      "File \u001b[1;32mc:\\Users\\Singapore Athletic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Singapore Athletic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\excel\\_base.py:364\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    363\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    365\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    366\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    367\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    368\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    369\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Singapore Athletic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1191\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1191\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1192\u001b[0m         content_or_path\u001b[39m=\u001b[39;49mpath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m   1193\u001b[0m     )\n\u001b[0;32m   1194\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1195\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1196\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1197\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1198\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Singapore Athletic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1070\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1068\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1070\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1071\u001b[0m     content_or_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1072\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m   1073\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   1074\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Singapore Athletic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:710\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    701\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    702\u001b[0m             handle,\n\u001b[0;32m    703\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    706\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    707\u001b[0m         )\n\u001b[0;32m    708\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    711\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    713\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'UniqueNamesRev24.xlsx'"
     ]
    }
   ],
   "source": [
    "UniqueNames=pd.read_excel('NameVariationDict/UniqueNamesRev24.xlsx')\n",
    "UniqueNames=UniqueNames.fillna('NIL')\n",
    "UniqueNames.reset_index()\n",
    "newdf=dffnln.copy() #Copy the dataframe to prevent any alteration to original file\n",
    "counter=0 #To see the progress of the code\n",
    "toreplace=[]\n",
    "\n",
    "#Create a column called CheckNames to read all the names without the commas. Make all captial also\n",
    "newdf['CheckNames']=newdf['NAME'].replace(',', '', regex=True)\n",
    "newdf['CheckNames']=newdf['CheckNames'].str.upper()\n",
    "newdf['CheckNames']=newdf['CheckNames'].str.strip()\n",
    "print(len(newdf))\n",
    "\n",
    "\n",
    "\n",
    "for names in newdf['CheckNames']:\n",
    "    if counter%500==0:\n",
    "        print(f'{counter} out of {len(newdf[\"CheckNames\"])} done.')\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        if UniqueNames['Actual Names'].str.contains(names).any():\n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Actual Names'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var1'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var1'] == names].index\n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var1'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var2'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var2'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var2'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var3'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var3'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var3'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var4'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var4'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var4'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var5'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var5'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var5'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var6'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var6'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var6'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var7'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var7'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var7'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var8'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var8'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var8'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var9'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var9'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var9'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var10'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var10'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var10'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var11'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var11'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var11'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var12'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var12'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var12'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var13'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var13'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var13'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var14'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var14'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var14'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var15'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var15'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var16'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var16'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var17'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var17'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var18'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var18'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var19'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var19'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var20'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var20'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var21'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var21'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var22'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var22'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var23'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var23'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var24'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var24'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var25'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var25'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var26'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var26'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var27'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var27'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var28'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var28'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var29'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var29'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var30'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var30'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var31'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var31'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var32'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var32'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var33'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var33'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var34'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var34'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var35'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var35'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var36'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var36'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        elif UniqueNames['Var37'].str.contains(names).any():\n",
    "            # row_num = UniqueNames[UniqueNames['Var15'] == names].index \n",
    "            row_num=[i for i, x in enumerate(UniqueNames['Var37'].str.contains(names)) if x]\n",
    "            toreplace.append(UniqueNames['Actual Names'][row_num[0]])\n",
    "        else:\n",
    "            toreplace.append('NoMatch')\n",
    "        counter+=1\n",
    "    except:\n",
    "        toreplace.append('Error')\n",
    "        counter+=1\n",
    "\n",
    "#Replace all the original names to the correct names\n",
    "newdf['NAME EDITED']=toreplace\n",
    "\n",
    "#Remember to rename the file everytime it is produced\n",
    "newdf.to_excel('CONSOLIDATED.xlsx')\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UniqueNames['Actual Names'].str.contains('TAN ELIZABETH-ANN').any()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
